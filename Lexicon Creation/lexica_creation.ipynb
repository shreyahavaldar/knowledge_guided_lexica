{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "diverse-parcel",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import fasttext as fastText\n",
    "from wordfreq import word_frequency, zipf_frequency\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d4f17",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e08421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext():\n",
    "    #load pretrained fasttext crawl-300d-2M-subword.zip\n",
    "    print('loading FT word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    f = open('/sandata/lexica_creation/crawl-300d-2M.vec',encoding='utf-8')\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx != 0:\n",
    "            values = line.strip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "fasttext = load_fasttext()\n",
    "\n",
    "vectors = fastText.load_model('/sandata/lexica_creation/wiki.en.bin')\n",
    "wiki_en_words, freqs = vectors.get_words(include_freq=True)\n",
    "wiki_en_freqs = dict(zip(wiki_en_words, freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3bd33",
   "metadata": {},
   "source": [
    "### Input seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each category should be a list of lowercase, stripped words\n",
    "def load_seed_words(file):\n",
    "    seed_words = []\n",
    "    with open(file,'r') as file:\n",
    "        for word in file:      \n",
    "            seed_words.append(word.lower().strip()) \n",
    "    return seed_words\n",
    "\n",
    "seed_df = pd.read_csv(\"TL_NHB.csv\")\n",
    "seed_words_indv = seed_df[seed_df[\"category\"] == \"indv\"][\"term\"].tolist()\n",
    "seed_words_coll = seed_df[seed_df[\"category\"] == \"coll\"][\"term\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-beginning",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: list of seed words\n",
    "# output: list of corresponding embeddings\n",
    "def get_seed_embeddings(ls):\n",
    "    ls_seed_emb = []\n",
    "    for seed in ls:\n",
    "        seed_emb = np.mean([fasttext[x] for x in seed.split(' ') if x in fasttext], axis = 0)\n",
    "        ls_seed_emb.append(seed_emb)\n",
    "    return ls_seed_emb\n",
    "\n",
    "# input: list of embeddings\n",
    "# output: center embedding\n",
    "def get_center_embedding(ls_emb):\n",
    "    # filter out embeddings with nan values\n",
    "    ls_emb = [x for x in ls_emb if not np.isnan(x).any()]\n",
    "    return np.mean(ls_emb, axis = 0)\n",
    "\n",
    "\n",
    "# input: center embedding\n",
    "# output: list of all words with their corresponding cosine similarity, descending order\n",
    "def concept_expansion(seed_center_emb,output_size):\n",
    "    final_list = []\n",
    "    min_sim = 0   \n",
    "    all_words = list(fasttext.keys())\n",
    "    for word in tqdm(all_words):\n",
    "        try:\n",
    "            cos_sim = cosine_similarity([seed_center_emb],[fasttext[word]])[0][0]\n",
    "        except:\n",
    "            print(\"error: \", word)\n",
    "            continue\n",
    "        if cos_sim > min_sim:\n",
    "            if len(final_list) == output_size:\n",
    "                final_list = final_list[:-1]\n",
    "            final_list.append((word,cos_sim))\n",
    "            final_list.sort(key = lambda x: x[1], reverse = True)\n",
    "            min_sim = final_list[-1][1]\n",
    "    return final_list\n",
    "\n",
    "\n",
    "# input: list of seed words, output size\n",
    "# output: concept expanded lexica\n",
    "def get_concept_expansion(seed_words, output_size):\n",
    "    ls_seed_emb = get_seed_embeddings(seed_words)\n",
    "    seed_center_emb = get_center_embedding(ls_seed_emb)\n",
    "    lexica = concept_expansion(seed_center_emb,output_size)\n",
    "    return lexica\n",
    "\n",
    "# input: seed word, fasttext model, number of synonyms\n",
    "# output: list of synonyms\n",
    "def synonym_expansion(word, model, k=50):\n",
    "    nearest_words = model.get_nearest_neighbors(word, k)\n",
    "    final_words = []\n",
    "    for n in nearest_words:\n",
    "        final_words.append((n[1], n[0]))\n",
    "    return final_words\n",
    "\n",
    "def save_dict_to_file(dic, filename):\n",
    "    f = open(filename,'w')\n",
    "    f.write(str(dic))\n",
    "    f.close()\n",
    "\n",
    "def load_dict_from_file(filename):\n",
    "    f = open(filename,'r')\n",
    "    data=f.read()\n",
    "    f.close()\n",
    "    return eval(data)\n",
    "\n",
    "def parse_list(lexica, threshold_wiki, threshold_wordfreq):\n",
    "    final_lexica = []\n",
    "    for w in lexica:\n",
    "        word = w[0]#.replace(\"-\", \" \")\n",
    "        try:\n",
    "            freq_wiki = wiki_en_freqs[word]\n",
    "        except:\n",
    "            freq_wiki = 0\n",
    "        freq_wordfreq = word_frequency(word, 'en', wordlist='best', minimum=0.0)\n",
    "        if(freq_wiki > threshold_wiki or freq_wordfreq > threshold_wordfreq):\n",
    "            final_lexica.append(w)\n",
    "    return final_lexica\n",
    "\n",
    "def print_excel(lexica):\n",
    "    for w in lexica:\n",
    "        print(w[0] + \", \" + str(w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-sector",
   "metadata": {},
   "source": [
    "## Lexica Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-capital",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVED = False\n",
    "\n",
    "if(SAVED):\n",
    "    synonym_expansion_indv = load_dict_from_file(\"nearest_words_indv.txt\")\n",
    "    synonym_expansion_coll = load_dict_from_file(\"nearest_words_coll.txt\")\n",
    "    concept_expansion_indv = load_dict_from_file(\"lexica_indv.txt\")\n",
    "    concept_expansion_coll = load_dict_from_file(\"lexica_coll.txt\")\n",
    "else:\n",
    "    #get synonyms for indvness words\n",
    "    synonym_expansion_indv = {}\n",
    "    for w in seed_words_indv:\n",
    "        print(w)\n",
    "        nearest_words = synonym_expansion(w, vectors, 100)\n",
    "        synonym_expansion_indv[w] = nearest_words\n",
    "    save_dict_to_file(synonym_expansion_indv, \"nearest_words_indv.txt\")\n",
    "\n",
    "    #get synonyms for collness words\n",
    "    synonym_expansion_coll = {}\n",
    "    for w in seed_words_coll:\n",
    "        print(w)\n",
    "        nearest_words = synonym_expansion(w, vectors, 100)\n",
    "        synonym_expansion_coll[w] = nearest_words\n",
    "    save_dict_to_file(synonym_expansion_coll, \"nearest_words_coll.txt\")\n",
    "\n",
    "    #get concept expansion for indvness words\n",
    "    concept_expansion_indv = get_concept_expansion(seed_words_indv,1000)\n",
    "    save_dict_to_file(concept_expansion_indv, \"lexica_indv.txt\")\n",
    "    \n",
    "    #get concept expansion for collness words\n",
    "    concept_expansion_coll = get_concept_expansion(seed_words_coll,1000)\n",
    "    save_dict_to_file(concept_expansion_coll, \"lexica_coll.txt\")    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d18b08",
   "metadata": {},
   "source": [
    "### Overlap synonym expansion and concept expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0809cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate results\n",
    "total_indv = []\n",
    "for seed in synonym_expansion_indv:\n",
    "    for word in synonym_expansion_indv[seed]:\n",
    "        total_indv.append(word)\n",
    "\n",
    "total_coll = []\n",
    "for seed in synonym_expansion_coll:\n",
    "    for word in synonym_expansion_coll[seed]:\n",
    "        total_coll.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_SINGLE = 0.75\n",
    "THRESHOLD_CLUSTER = 0.45\n",
    "\n",
    "final_indv = []\n",
    "#Add seed words to final lexica with distance 1\n",
    "for w in seed_words_indv:\n",
    "    final_indv.append((w, 1))\n",
    "\n",
    "#add words from single word cluster expansion that also overlap with joint cluster expansion\n",
    "#where distance from expanded word --> original word is > THRESHOLD_SINGLE\n",
    "final_temp_keys = list(zip(*final_indv))[0]\n",
    "lexica_indv_keys = list(zip(*concept_expansion_indv))[0]\n",
    "for w1 in total_indv:\n",
    "    if (w1[0] in lexica_indv_keys and w1[1] > THRESHOLD_SINGLE and w1[0] not in final_temp_keys):\n",
    "        final_indv.append(w1)\n",
    "\n",
    "#add words from joint cluster expansion \n",
    "#where distance from expanded word --> original centroid is > THRESHOLD_CLUSTER\n",
    "final_temp_keys = list(zip(*final_indv))[0]\n",
    "for w in concept_expansion_indv:\n",
    "    if(w[1] > THRESHOLD_CLUSTER and w[0] not in final_temp_keys):\n",
    "        final_indv.append(w)\n",
    "\n",
    "final_coll = []\n",
    "#Add seed words to final lexica with distance 1\n",
    "for w in seed_words_coll:\n",
    "    final_coll.append((w, 1))\n",
    "\n",
    "#add words from single word cluster expansion that also overlap with joint cluster expansion\n",
    "#where distance from expanded word --> original word is > THRESHOLD_SINGLE\n",
    "final_temp_keys = list(zip(*final_coll))[0]\n",
    "lexica_coll_keys = list(zip(*concept_expansion_coll))[0]\n",
    "for w1 in total_coll:\n",
    "    if (w1[0] in lexica_coll_keys and w1[1] > THRESHOLD_SINGLE and w1[0] not in final_temp_keys):\n",
    "        final_coll.append(w1)\n",
    "\n",
    "#add words from joint cluster expansion \n",
    "#where distance from expanded word --> original centroid is > THRESHOLD_CLUSTER\n",
    "final_temp_keys = list(zip(*final_coll))[0]\n",
    "for w in concept_expansion_coll:\n",
    "    if(w[1] > THRESHOLD_CLUSTER and w[0] not in final_temp_keys):\n",
    "        final_coll.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15751241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words with tokenizing errors\n",
    "THRESHOLD_WIKI = 50\n",
    "THRESHOLD_WORDFREQ = 1e-4\n",
    "\n",
    "final_indv_parsed = parse_list(final_indv, THRESHOLD_WIKI, THRESHOLD_WORDFREQ)\n",
    "final_coll_parsed = parse_list(final_coll, THRESHOLD_WIKI, THRESHOLD_WORDFREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save file to csv\n",
    "FILENAME = \"individualism_collectivism.csv\".format(THRESHOLD_CLUSTER, THRESHOLD_SINGLE)\n",
    "df = pd.DataFrame(columns=['WORD', 'CATEGORY','WEIGHT'])\n",
    "for w in final_indv_parsed:\n",
    "    df = df.append({'WORD': w[0], 'CATEGORY': 'indv', 'WEIGHT': w[1]}, ignore_index=True)\n",
    "for w in final_coll_parsed:\n",
    "    df = df.append({'WORD': w[0], 'CATEGORY': 'coll', 'WEIGHT': w[1]}, ignore_index=True)\n",
    "df.to_csv(\"{}\".format(FILENAME), index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecdbd8d05ba495d4a41ea56b380eb3c348934e8aa7cd13271a79d86ff4355567"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
